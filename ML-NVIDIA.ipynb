{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning & Tech Stocks: NVIDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the training set\n",
    "url = '~/wd/GitHub/TechStocks/NVDA.csv'\n",
    "dataset_train = pd.read_csv(url)\n",
    "training_set = dataset_train.iloc[:, 1:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-14</td>\n",
       "      <td>36.680000</td>\n",
       "      <td>37.865002</td>\n",
       "      <td>36.442501</td>\n",
       "      <td>37.610001</td>\n",
       "      <td>37.317863</td>\n",
       "      <td>73016800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-15</td>\n",
       "      <td>37.939999</td>\n",
       "      <td>38.337502</td>\n",
       "      <td>37.282501</td>\n",
       "      <td>37.467499</td>\n",
       "      <td>37.176472</td>\n",
       "      <td>61701200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>37.742500</td>\n",
       "      <td>38.075001</td>\n",
       "      <td>37.154999</td>\n",
       "      <td>37.209999</td>\n",
       "      <td>36.920959</td>\n",
       "      <td>47010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>36.877499</td>\n",
       "      <td>38.332500</td>\n",
       "      <td>36.602501</td>\n",
       "      <td>37.930000</td>\n",
       "      <td>37.635380</td>\n",
       "      <td>49343600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-18</td>\n",
       "      <td>38.432499</td>\n",
       "      <td>39.494999</td>\n",
       "      <td>37.912498</td>\n",
       "      <td>39.232498</td>\n",
       "      <td>38.927757</td>\n",
       "      <td>65133600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close  Adj Close    Volume\n",
       "0  2019-01-14  36.680000  37.865002  36.442501  37.610001  37.317863  73016800\n",
       "1  2019-01-15  37.939999  38.337502  37.282501  37.467499  37.176472  61701200\n",
       "2  2019-01-16  37.742500  38.075001  37.154999  37.209999  36.920959  47010400\n",
       "3  2019-01-17  36.877499  38.332500  36.602501  37.930000  37.635380  49343600\n",
       "4  2019-01-18  38.432499  39.494999  37.912498  39.232498  38.927757  65133600"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview dataset\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1259"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization\n",
    "We normalize the data by changing the values of numeric columns in the dataset to a common scale, which helps the performance of the model. Then to scale the training dataset, use Scikit-Learnâ€™s MinMaxScaler with numbers between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating Timesteps Into Data\n",
    "Input our data in the form of a 3D array to the LSTM model by first creating data in 60 timesteps before using numpy to convert it into an array, then convert the data into a 3D array with X_train samples, 60 timestamps, and one feature at each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure with 60 timesteps and 1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(60, 1259): # 60 is the number of timesteps\n",
    "    X_train.append(training_set_scaled[i-60:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1)) # Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM layer is added with the following arguments: 50 units is the dimensionality of the output space, return_sequences=True is necessary for stacking LSTM layers so the consequent LSTM layer has a three-dimensional sequence input, and input_shape is the shape of the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying 0.2 in the Dropout layer means that 20% of the layers will be dropped. Following the LSTM and Dropout layers, we add the Dense layer that specifies an output of one unit. To compile our model we use the Adam optimizer and set the loss as the mean_squared_error. After that, we fit the model to run for 100 epochs (the epochs are the number of times the learning algorithm will work through the entire training set) with a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "38/38 [==============================] - 14s 90ms/step - loss: 0.0161\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 3s 88ms/step - loss: 0.0038\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 0.0040\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 0.0032\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0033\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 4s 94ms/step - loss: 0.0033\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 3s 86ms/step - loss: 0.0031\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 3s 86ms/step - loss: 0.0027\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 0.0027\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 0.0031\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 0.0025\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 3s 90ms/step - loss: 0.0027\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 3s 88ms/step - loss: 0.0025\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 3s 87ms/step - loss: 0.0024\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0023\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 4s 93ms/step - loss: 0.0029\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 3s 90ms/step - loss: 0.0021\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 3s 89ms/step - loss: 0.0024\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 4s 93ms/step - loss: 0.0022\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 3s 85ms/step - loss: 0.0018\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0018\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 3s 91ms/step - loss: 0.0018\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 3s 90ms/step - loss: 0.0017\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 3s 86ms/step - loss: 0.0016\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 3s 90ms/step - loss: 0.0018\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 4s 93ms/step - loss: 0.0015\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 3s 90ms/step - loss: 0.0019\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 3s 87ms/step - loss: 0.0017\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 3s 91ms/step - loss: 0.0017\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 3s 90ms/step - loss: 0.0015\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 3s 89ms/step - loss: 0.0017\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 4s 99ms/step - loss: 0.0016\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 3s 89ms/step - loss: 0.0015\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 0.0014\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 0.0016\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 3s 79ms/step - loss: 0.0016\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 3s 76ms/step - loss: 0.0014\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 3s 75ms/step - loss: 0.0015\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 3s 79ms/step - loss: 0.0013\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 3s 77ms/step - loss: 0.0015\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 0.0012\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 3s 79ms/step - loss: 0.0015\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 3s 78ms/step - loss: 0.0017\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 3s 78ms/step - loss: 0.0011\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 3s 80ms/step - loss: 0.0013\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 3s 73ms/step - loss: 0.0012\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 3s 78ms/step - loss: 0.0013\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 3s 79ms/step - loss: 0.0015\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 0.0014\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 0.0015\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 3s 86ms/step - loss: 0.0015\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 3s 81ms/step - loss: 0.0015\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 3s 75ms/step - loss: 0.0013\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 3s 77ms/step - loss: 0.0012\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 3s 74ms/step - loss: 0.0013\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 3s 77ms/step - loss: 0.0011\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 3s 80ms/step - loss: 0.0012\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 3s 74ms/step - loss: 0.0013\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 3s 75ms/step - loss: 0.0011\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 3s 74ms/step - loss: 0.0012\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 3s 80ms/step - loss: 0.0013\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 3s 79ms/step - loss: 0.0012\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 3s 74ms/step - loss: 0.0012\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 3s 80ms/step - loss: 0.0012\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 0.0013\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 3s 86ms/step - loss: 0.0012\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0010\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 3s 92ms/step - loss: 0.0013\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0015\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 3s 90ms/step - loss: 0.0011\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 3s 89ms/step - loss: 0.0011\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 3s 86ms/step - loss: 0.0012\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0011\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 3s 91ms/step - loss: 0.0011\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 3s 78ms/step - loss: 0.0012\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 3s 80ms/step - loss: 0.0010\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 3s 90ms/step - loss: 0.0011\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 3s 79ms/step - loss: 0.0011\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 3s 78ms/step - loss: 0.0011\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 3s 75ms/step - loss: 0.0011\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 3s 78ms/step - loss: 0.0011\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 3s 79ms/step - loss: 0.0011\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 3s 77ms/step - loss: 9.0110e-04\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 3s 76ms/step - loss: 0.0010\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 3s 87ms/step - loss: 9.8769e-04\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 0.0013\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 3s 75ms/step - loss: 0.0012\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 3s 78ms/step - loss: 0.0011\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 0.0010\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 3s 76ms/step - loss: 9.6251e-04\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 3s 77ms/step - loss: 0.0010\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 3s 74ms/step - loss: 0.0010\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 3s 77ms/step - loss: 0.0011\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 3s 75ms/step - loss: 9.0884e-04\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 3s 74ms/step - loss: 0.0011\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 3s 81ms/step - loss: 8.2885e-04\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 9.0842e-04\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 3s 76ms/step - loss: 9.4537e-04\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 0.0013\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 3s 87ms/step - loss: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x14380f2d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=50,return_sequences=True,input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1))\n",
    "model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "model.fit(X_train,y_train,epochs=100,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions from the Test Set\n",
    "\n",
    "Before predicting future stock prices, modify the test set (notice similarities to the edits made to the training set): merge the training set and the test set on the 0 axis, set 60 as the time step again, use MinMaxScaler, and reshape data. Then, inverse_transform puts the stock prices in a normal readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the test set\n",
    "url = 'https://raw.githubusercontent.com/mwitiderrick/stockprice/master/tatatest.csv'\n",
    "dataset_test = pd.read_csv(url)\n",
    "real_stock_price = dataset_test.iloc[:, 1:2].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
